---
title: "lab6"
format: html
---

## Lab Set Up

```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
```

## Data Download

```{r}
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
```

## Getting the Documentation PDF

```{r}
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf', mode = "wb")
```

## Getting Basin Characteristics

```{r}
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
```

```{r}
remote_files  <- glue('{root}/camels_{types}.txt')

local_files   <- glue('data/camels_{types}.txt')
```

```{r}
walk2(remote_files, local_files, download.file, quiet = TRUE)
```

```{r}
camels <- map(local_files, read_delim, show_col_types = FALSE) 
```

```{r}
camels <- power_full_join(camels ,by = 'gauge_id')
```

# Question 1:

From the documentation PDF, *zero_q_freq* represents the frequency of days where streamflow (Q) is 0 mm per day, meaning no measurable water flow was recorded.

## Exploratory Data Analysis:

```{r}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()
```

# Question 2:

```{r}
library(ggplot2)
library(ggthemes)
library(patchwork)
library(sf)
library(viridis)
```

```{r}
camels_sf <- st_as_sf(camels, coords = c("gauge_lon", "gauge_lat"), crs = 4326)

map_aridity <- ggplot(data = camels_sf) +
  geom_sf(aes(color = aridity), size = 2) +
  scale_color_viridis(option = "C", name = "Aridity") +
  labs(title = "Sites Colored by Aridity", x = "Longitude", y = "Latitude") +
  theme_minimal()

map_p_mean <- ggplot(data = camels_sf) +
  geom_sf(aes(color = p_mean), size = 2) +
  scale_color_viridis(option = "D", name = "Mean Precipitation (mm)") +
  labs(title = "Sites Colored by Mean Precipitation", x = "Longitude", y = "Latitude") +
  theme_minimal()

combined_map <- map_aridity + map_p_mean + plot_layout(ncol = 1)

print(combined_map)
```

## Model preparation:

```{r}
camels |> 
  select(aridity, p_mean, q_mean) |> 
  drop_na() |> 
  cor()
```

## Visual EDA:

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm", color = "red", linetype = 2) +
  scale_color_viridis_c() +
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

## Testing a transformation:

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

## Log transform color scale

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow") 
```

## Splitting the data:

```{r}
set.seed(123)

camels <- camels |> 
  mutate(logQmean = log(q_mean))

camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)
```

## Create a recipe to preprocess data:

```{r}
rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  step_log(all_predictors()) %>%
  step_interact(terms = ~ aridity:p_mean) |> 
  step_naomit(all_predictors(), all_outcomes())
```

## Native base *lm* approach:

```{r}
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)
```

```{r}
summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))
```

```{r}
test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)
```

## Statistical and Visual Evaluation:

```{r}
metrics(test_data, truth = logQmean, estimate = lm_pred)
```

```{r}
ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +
  scale_color_gradient2(low = "brown", mid = "orange", high = "darkgreen") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")
```

```{r}
lm_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

lm_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lm_model) %>%
  fit(data = camels_train) 

summary(extract_fit_engine(lm_wf))$coefficients
```

```{r}
summary(lm_base)$coefficients
```

## Making predictions:

```{r}
lm_data <- augment(lm_wf, new_data = camels_test)
dim(lm_data)
```

## Statistical and Visual Evaluation:

```{r}
metrics(lm_data, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
library(baguette)
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_model) %>%
  fit(data = camels_train) 
```

## Predictions:

```{r}
rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)
```

## Statistical and Visual Evaluation:

```{r}
ggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

## A *workflowset* approach:

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)
```

```{r}
rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```

# Question 3:

## Build *xgboost* regression model:

```{r}
library(xgboost)

xgb_model <- boost_tree(trees = 1000, tree_depth = 6, learn_rate = 0.1) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(xgb_model) %>%
  fit(data = camels_train)
```

```{r}
xgb_data <- augment(xgb_wf, new_data = camels_test)

ggplot(xgb_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

## Build a neural network model:

```{r}
nn_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")

nn_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(nn_model) %>%
  fit(data = camels_train)
```

## Evaluate the model:

```{r}
nn_data <- augment(nn_wf, new_data = camels_test)

ggplot(nn_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

## Comparing all models:

```{r}
xg_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xg_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(xg_model) %>%
  fit(data = camels_train)

nn_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")

nn_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(nn_model) %>%
  fit(data = camels_train)


lm_data <- augment(lm_wf, new_data = camels_test)

rf_data <- augment(rf_wf, new_data = camels_test)

xgb_data <- augment(xgb_wf, new_data = camels_test)

nn_data <- augment(nn_wf, new_data = camels_test)

```

## Evaluate Model Performance:
```{r}
library(yardstick)

evaluate_model <- function(data) {
  metrics(data, truth = logQmean, estimate = .pred)
}

lm_metrics <- evaluate_model(lm_data) %>% mutate(model = "Linear Model")

rf_metrics <- evaluate_model(rf_data) %>% mutate(model = "Random Forest")

xgb_metrics <- evaluate_model(xgb_data) %>% mutate(model = "XGBoost")

nn_metrics <- evaluate_model(nn_data) %>% mutate(model = "Neural Network")

model_comparison <- bind_rows(lm_metrics, rf_metrics, xgb_metrics, nn_metrics)

print(model_comparison)
```

## Visual Evaluation:
```{r}
ggplot(model_comparison, aes(x = model, y = .estimate, fill = .metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_minimal() +
  labs(title = "Model Performance Comparison", y = "Metric Value", x = "Model")
```

## Which of the 4 models would you move forward with?

When examining all three of the metrics used to evaluate the models (RMSE, R2, and MAE), there is a clear trend that the neural network model is the most accurate. I would move forward with the neural net model because it has the lowest RMSE (more accurate predictions), highest R2 (better model fit), and the lowest MAE (less error in predictions). While the gains aren't extreme, the trend is clearly indicating that the neural network model is the most accurate of the four, and should be the one we move forward with.

# Build Your Own

## Data Splitting:

```{r}
# Set a seed for reproducible:
set.seed(123)

# Split 75% training/25% testing
split_data <- initial_split(camels, prop = 0.75)

# Extract training/testing sets:
train_data <- training(split_data)
test_data <- testing(split_data)

# 10-fold CV dataset:
cv_splits <- vfold_cv(train_data, v = 10)
```

## Recipe:

```{r}
# Define the formula for logQmean:
formula <- logQmean ~ p_mean + pet_mean + elev_mean + area_gages2 + max_water_content + slope_mean
```

### Describe why you're choosing this formula:

I chose to use the formula above because it captures variables that are all important in predicting streamflow, including precipitation, PET, catchment elevation, catchment area, catchment slope, and maximum water content. Precipitation and potential evapotranspiration all influence the water availability in an area and how much water would be readily available within the streamflow. Catchment elevation, slope, and area are all significantly important characteristics of a region that influence its access to water, availability of rain/snow, exposure to sun and evaporation, the types of plants absorbing water, etc. Each of the characteristics selected have significant tangible impacts on streamflow in different regions.

```{r}
# Build a recipe:
rec <- recipe(formula, data = camels_train) %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors())
```

## Define 3 models:

### Random forest model:
```{r}
library(parsnip)

rf_camel <- rand_forest(trees = 1000, mtry = 4, min_n = 5) %>%
  set_engine("ranger") %>%
  set_mode("regression")
```

### XGBoost model:
```{r}
xgb_camel <- boost_tree(trees = 1000, tree_depth = 6, learn_rate = 0.1) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
```

### Neural network model:
```{r}
nn_camel <- bag_mlp(hidden_units = 3, penalty = 0.01) %>%
  set_engine("nnet") %>%
  set_mode("regression")
```

## Workflow set()
```{r}
# Create a workflow object, add recipe, add models, and fit to resamples:
wf_2 <- workflow_set(list(rec), list(xgb_camel, rf_camel, nn_camel)) %>%
  workflow_map('fit_resamples', resamples = camels_cv)
```

## Evaluation:
```{r}
autoplot(wf_2)
```

```{r}
ranked_results <- wf_2 %>%
  rank_results()

print(ranked_results)
```

The model that is best from this selection is the neural network model, which is the only model that yields an R2 value that is over 0.9. Thus, using the lab's metric that a successful model will have an R-squared value greater than 0.9, the neural network model is the only one of my three tested models that is fully successful. It also has the lowest RMSE value, meaning the model's predictions are closer to the actual values than the predicted values of the boost_tree and rand_forest models.

## Extract and Evaluate:

```{r}
# Build a workflow with the model:
nn_wf <- workflow() %>%
  add_model(nn_camel) %>%
  add_recipe(rec)

# Fit the data to the model:
nn_fit <- fit(nn_wf, data = train_data)

# Make predictions on the test data:
library(broom)
nn_preds <- augment(nn_fit, new_data = test_data)

# Plot observed vs. predicted values
ggplot(nn_preds, aes(x = .pred, y = logQmean)) +
  geom_point(aes(color = .pred), alpha = 0.7) +
  geom_abline(slope = 1, intercept = 0, color = "black", linetype = "dashed") +
  scale_color_viridis_c() +
  labs(
    title = "Neural Network Model: Predicted vs. Observed logQmean",
    x = "Predicted logQmean",
    y = "Observed logQmean",
    color = "Predicted"
  ) +
  theme_minimal()
```

The predicted vs. observed logQmean plot above shows a fairly tight clustering of points around the 1:1 line, indicating that the neural network model is making accurate predictions on the test data. The color gradient helps illustrate and draw attention to where the predicted values fall. While there is some spread in the upper and lower tails, the overall pattern suggests that the model pretty accurately captures the main drivers of streamflow. I think these findings are supportive of choosing the neural network for modeling logQmean in this dataset.