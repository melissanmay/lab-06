{
  "hash": "887e5478b2f14efd27221e05aba85c1d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 8: Machine Learning Tuning\"\nauthor: \"Melissa May\"\ndate: \"2025-04-11\"\nformat: html\nexecute: \n  echo: true\n---\n\n\n\n# Libraries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(purrr)\nlibrary(skimr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'skimr' was built under R version 4.4.3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(visdat)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'visdat' was built under R version 4.4.3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggpubr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggpubr' was built under R version 4.4.3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(glue)\nlibrary(powerjoin)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'powerjoin' was built under R version 4.4.3\n```\n\n\n:::\n:::\n\n\n\n# Data Import/Tidy/Transform\n\n## Read in the data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroot  <- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf', mode = \"wb\")\n\ntypes <- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  <- glue('{root}/camels_{types}.txt')\n\nlocal_files   <- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels <- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels <- power_full_join(camels ,by = 'gauge_id')\n```\n:::\n\n\n\n## Data cleaning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvis_dat(camels)\n```\n\n::: {.cell-output-display}\n![](hyperparameter-tuning_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nskim(camels)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |       |\n|:------------------------|:------|\n|Name                     |camels |\n|Number of rows           |671    |\n|Number of columns        |58     |\n|_______________________  |       |\n|Column type frequency:   |       |\n|character                |6      |\n|numeric                  |52     |\n|________________________ |       |\n|Group variables          |None   |\n\n\n**Variable type: character**\n\n|skim_variable    | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:----------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|gauge_id         |         0|          1.00|   8|   8|     0|      671|          0|\n|high_prec_timing |         0|          1.00|   3|   3|     0|        4|          0|\n|low_prec_timing  |         0|          1.00|   3|   3|     0|        4|          0|\n|geol_1st_class   |         0|          1.00|  12|  31|     0|       12|          0|\n|geol_2nd_class   |       138|          0.79|  12|  31|     0|       13|          0|\n|dom_land_cover   |         0|          1.00|  12|  38|     0|       12|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable        | n_missing| complete_rate|   mean|      sd|      p0|     p25|    p50|    p75|     p100|hist  |\n|:--------------------|---------:|-------------:|------:|-------:|-------:|-------:|------:|------:|--------:|:-----|\n|p_mean               |         0|          1.00|   3.26|    1.41|    0.64|    2.37|   3.23|   3.78|     8.94|▃▇▂▁▁ |\n|pet_mean             |         0|          1.00|   2.79|    0.55|    1.90|    2.34|   2.69|   3.15|     4.74|▇▇▅▂▁ |\n|p_seasonality        |         0|          1.00|  -0.04|    0.53|   -1.44|   -0.26|   0.08|   0.22|     0.92|▁▂▃▇▂ |\n|frac_snow            |         0|          1.00|   0.18|    0.20|    0.00|    0.04|   0.10|   0.22|     0.91|▇▂▁▁▁ |\n|aridity              |         0|          1.00|   1.06|    0.62|    0.22|    0.70|   0.86|   1.27|     5.21|▇▂▁▁▁ |\n|high_prec_freq       |         0|          1.00|  20.93|    4.55|    7.90|   18.50|  22.00|  24.23|    32.70|▂▃▇▇▁ |\n|high_prec_dur        |         0|          1.00|   1.35|    0.19|    1.08|    1.21|   1.28|   1.44|     2.09|▇▅▂▁▁ |\n|low_prec_freq        |         0|          1.00| 254.65|   35.12|  169.90|  232.70| 255.85| 278.92|   348.70|▂▅▇▅▁ |\n|low_prec_dur         |         0|          1.00|   5.95|    3.20|    2.79|    4.24|   4.95|   6.70|    36.51|▇▁▁▁▁ |\n|glim_1st_class_frac  |         0|          1.00|   0.79|    0.20|    0.30|    0.61|   0.83|   1.00|     1.00|▁▃▃▃▇ |\n|glim_2nd_class_frac  |         0|          1.00|   0.16|    0.14|    0.00|    0.00|   0.14|   0.27|     0.49|▇▃▃▂▁ |\n|carbonate_rocks_frac |         0|          1.00|   0.12|    0.26|    0.00|    0.00|   0.00|   0.04|     1.00|▇▁▁▁▁ |\n|geol_porostiy        |         3|          1.00|   0.13|    0.07|    0.01|    0.07|   0.13|   0.19|     0.28|▇▆▇▇▂ |\n|geol_permeability    |         0|          1.00| -13.89|    1.18|  -16.50|  -14.77| -13.96| -13.00|   -10.90|▂▅▇▅▂ |\n|soil_depth_pelletier |         0|          1.00|  10.87|   16.24|    0.27|    1.00|   1.23|  12.89|    50.00|▇▁▁▁▁ |\n|soil_depth_statsgo   |         0|          1.00|   1.29|    0.27|    0.40|    1.11|   1.46|   1.50|     1.50|▁▁▂▂▇ |\n|soil_porosity        |         0|          1.00|   0.44|    0.02|    0.37|    0.43|   0.44|   0.46|     0.68|▃▇▁▁▁ |\n|soil_conductivity    |         0|          1.00|   1.74|    1.52|    0.45|    0.93|   1.35|   1.93|    13.96|▇▁▁▁▁ |\n|max_water_content    |         0|          1.00|   0.53|    0.15|    0.09|    0.43|   0.56|   0.64|     1.05|▁▅▇▃▁ |\n|sand_frac            |         0|          1.00|  36.47|   15.63|    8.18|   25.44|  35.27|  44.46|    91.98|▅▇▅▁▁ |\n|silt_frac            |         0|          1.00|  33.86|   13.25|    2.99|   23.95|  34.06|  43.64|    67.77|▂▆▇▆▁ |\n|clay_frac            |         0|          1.00|  19.89|    9.32|    1.85|   14.00|  18.66|  25.42|    50.35|▃▇▅▂▁ |\n|water_frac           |         0|          1.00|   0.10|    0.94|    0.00|    0.00|   0.00|   0.00|    19.35|▇▁▁▁▁ |\n|organic_frac         |         0|          1.00|   0.59|    3.84|    0.00|    0.00|   0.00|   0.00|    57.86|▇▁▁▁▁ |\n|other_frac           |         0|          1.00|   9.82|   16.83|    0.00|    0.00|   1.31|  11.74|    99.38|▇▁▁▁▁ |\n|gauge_lat            |         0|          1.00|  39.24|    5.21|   27.05|   35.70|  39.25|  43.21|    48.82|▂▃▇▆▅ |\n|gauge_lon            |         0|          1.00| -95.79|   16.21| -124.39| -110.41| -92.78| -81.77|   -67.94|▆▃▇▇▅ |\n|elev_mean            |         0|          1.00| 759.42|  786.00|   10.21|  249.67| 462.72| 928.88|  3571.18|▇▂▁▁▁ |\n|slope_mean           |         0|          1.00|  46.20|   47.12|    0.82|    7.43|  28.80|  73.17|   255.69|▇▂▂▁▁ |\n|area_gages2          |         0|          1.00| 792.62| 1701.95|    4.03|  122.28| 329.68| 794.30| 25791.04|▇▁▁▁▁ |\n|area_geospa_fabric   |         0|          1.00| 808.08| 1709.85|    4.10|  127.98| 340.70| 804.50| 25817.78|▇▁▁▁▁ |\n|frac_forest          |         0|          1.00|   0.64|    0.37|    0.00|    0.28|   0.81|   0.97|     1.00|▃▁▁▂▇ |\n|lai_max              |         0|          1.00|   3.22|    1.52|    0.37|    1.81|   3.37|   4.70|     5.58|▅▆▃▅▇ |\n|lai_diff             |         0|          1.00|   2.45|    1.33|    0.15|    1.20|   2.34|   3.76|     4.83|▇▇▇▆▇ |\n|gvf_max              |         0|          1.00|   0.72|    0.17|    0.18|    0.61|   0.78|   0.86|     0.92|▁▁▂▃▇ |\n|gvf_diff             |         0|          1.00|   0.32|    0.15|    0.03|    0.19|   0.32|   0.46|     0.65|▃▇▅▇▁ |\n|dom_land_cover_frac  |         0|          1.00|   0.81|    0.18|    0.31|    0.65|   0.86|   1.00|     1.00|▁▂▃▃▇ |\n|root_depth_50        |        24|          0.96|   0.18|    0.03|    0.12|    0.17|   0.18|   0.19|     0.25|▃▃▇▂▂ |\n|root_depth_99        |        24|          0.96|   1.83|    0.30|    1.50|    1.52|   1.80|   2.00|     3.10|▇▃▂▁▁ |\n|q_mean               |         1|          1.00|   1.49|    1.54|    0.00|    0.63|   1.13|   1.75|     9.69|▇▁▁▁▁ |\n|runoff_ratio         |         1|          1.00|   0.39|    0.23|    0.00|    0.24|   0.35|   0.51|     1.36|▆▇▂▁▁ |\n|slope_fdc            |         1|          1.00|   1.24|    0.51|    0.00|    0.90|   1.28|   1.63|     2.50|▂▅▇▇▁ |\n|baseflow_index       |         0|          1.00|   0.49|    0.16|    0.01|    0.40|   0.50|   0.60|     0.98|▁▃▇▅▁ |\n|stream_elas          |         1|          1.00|   1.83|    0.78|   -0.64|    1.32|   1.70|   2.23|     6.24|▁▇▃▁▁ |\n|q5                   |         1|          1.00|   0.17|    0.27|    0.00|    0.01|   0.08|   0.22|     2.42|▇▁▁▁▁ |\n|q95                  |         1|          1.00|   5.06|    4.94|    0.00|    2.07|   3.77|   6.29|    31.82|▇▂▁▁▁ |\n|high_q_freq          |         1|          1.00|  25.74|   29.07|    0.00|    6.41|  15.10|  35.79|   172.80|▇▂▁▁▁ |\n|high_q_dur           |         1|          1.00|   6.91|   10.07|    0.00|    1.82|   2.85|   7.55|    92.56|▇▁▁▁▁ |\n|low_q_freq           |         1|          1.00| 107.62|   82.24|    0.00|   37.44|  96.00| 162.14|   356.80|▇▆▅▂▁ |\n|low_q_dur            |         1|          1.00|  22.28|   21.66|    0.00|   10.00|  15.52|  26.91|   209.88|▇▁▁▁▁ |\n|zero_q_freq          |         1|          1.00|   0.03|    0.11|    0.00|    0.00|   0.00|   0.00|     0.97|▇▁▁▁▁ |\n|hfd_mean             |         1|          1.00| 182.52|   33.53|  112.25|  160.16| 173.77| 204.05|   287.75|▂▇▃▂▁ |\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncamels_clean <- camels %>%\n  drop_na(q_mean) %>%\n  select(gauge_id, gauge_lat, gauge_lon, everything())\n\nggpubr::ggdensity(camels_clean$q_mean,\n                  fill = \"mediumaquamarine\",\n                  rug = TRUE,\n                  xlab = \"Mean Streamflow (q_mean)\")\n```\n\n::: {.cell-output-display}\n![](hyperparameter-tuning_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n# Data Splitting\n\n## Set a seed\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n```\n:::\n\n\n\n## Split the data 80/20%\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncamels_split <- initial_split(camels_clean, prop = 0.8)\n```\n:::\n\n\n\n## Extract the training and testing sets\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncamels_train <- training(camels_split)\n\ncamels_test <- testing(camels_split)\n```\n:::\n\n\n\n# Feature Engineering\n\n## Create a recipe object\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncamels_recipe <- recipe(q_mean ~ ., data = camels_train) %>%\n  step_rm(gauge_lat, gauge_lon) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_unknown(all_nominal_predictors()) %>%\n  step_other(all_nominal_predictors(), threshold = 0.01) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n```\n:::\n\n\n\n# Resampling and Model Testing\n\n## Build resamples\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfolds <- vfold_cv(camels_train, v = 10)\n```\n:::\n\n\n\n## Build 3 candidate models\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlin_reg_model <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\nrf_model <- rand_forest(mtry = 5, trees = 500) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\nxgb_model <- boost_tree(trees = 500, learn_rate = 0.1) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n```\n:::\n\n\n\n## Test the models\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels <- list(\n  linear = lin_reg_model,\n  random_forest = rf_model,\n  xgboost = xgb_model\n)\n\nwf_set <- workflow_set(\n  preproc = list(camels_recipe),\n  models = models\n)\n\nset.seed(123)\nwf_results <- wf_set %>%\n  workflow_map(\"fit_resamples\", resamples = folds)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ranger' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'xgboost' was built under R version 4.4.3\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(wf_results)\n```\n\n::: {.cell-output-display}\n![](hyperparameter-tuning_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n## Model selection\n\nBased on the visualized metrics, the model I think will perform best is the XGBoost model. While both the boost_tree and linear_reg models performed very well, yielding low RMSE values and high R-squared values, the boost_tree model performed slightly better compared to the linear_reg model. I feel confident choosing the XGBoost model as opposed to the other two, because it has the lowest RMSE value and the highest R-squared value, meaning the confidence in its predictions is relatively high.\n\nThe model I selected, the XGBoost model, is a boosted tree model that used the \"xgboost\" engine and the \"regression\" mode. I think this model is performing especially well for this problem because boosted tree models work strongly with nonlinear data that simpler models can't capture as well. Given the presence of varying predictors and complex hydrological patterns, the boosted tree model is likely capturing these trends better than the other two models.\n\n# Model Tuning\n\n## Build a model for your chosen specification\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parsnip)\nlibrary(tune)\n\nboost_tree_tuned <- boost_tree(\n  mode = \"regression\",\n  engine = \"xgboost\",\n  trees = tune(),\n  tree_depth = tune(),\n  learn_rate = tune()\n)\n```\n:::\n\n\n\n## Create a workflow\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(workflows)\n\nwf_tune <- workflow() %>%\n  add_model(boost_tree_tuned) %>%\n  add_recipe(camels_recipe)\n```\n:::\n\n\n\n## Check the tunable values/ranges\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dials)\n\ndials <- extract_parameter_set_dials(wf_tune)\n\ndials$object\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n# Trees (quantitative)\nRange: [1, 2000]\n\n[[2]]\nTree Depth (quantitative)\nRange: [1, 15]\n\n[[3]]\nLearning Rate (quantitative)\nTransformer: log-10 [1e-100, Inf]\nRange (transformed scale): [-3, -0.5]\n```\n\n\n:::\n:::\n\n\n\n## Define the search space\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(finetune)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'finetune' was built under R version 4.4.3\n```\n\n\n:::\n\n```{.r .cell-code}\ngrid <- grid_latin_hypercube(\n  dials,\n  size = 25\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `grid_latin_hypercube()` was deprecated in dials 1.3.0.\nℹ Please use `grid_space_filling()` instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  trees tree_depth learn_rate\n  <int>      <int>      <dbl>\n1   578          6    0.00192\n2   842          1    0.0691 \n3  1290          4    0.0102 \n4  1152          9    0.00149\n5  1650          8    0.00379\n6   475         14    0.00714\n```\n\n\n:::\n:::\n\n\n\n## Tune the model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tune)\nlibrary(yardstick)\nlibrary(ggplot2)\n\nmodel_params <- tune_grid(\n  wf_tune,\n  resamples = folds,\n  grid = grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(model_params)\n```\n\n::: {.cell-output-display}\n![](hyperparameter-tuning_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\nEach row in the plot corresponds to a performance metric (mae, rmse, and rsq). Mean Absolute Error (MAE) quantifies the difference between predicted and actual values, and so lower MAE values mean the predictors are closer to the actual values. Root Mean Square Error (RMSE) is also a value quantifying the predictor's deviation from the actual observed values, and lower RMSE values are also indicative of better model performance. Finally, R-squared values quantify the proportion of variance in the dependent variable that can be explained by the independent variable. Thus, higher values for the r-squared (particularly R \\> 0.9), are indicative of an accurate model.\n\nEach column is one of the hyperparameters we tuned, including the number of boosting iterations, the log-scale learning rate, and the depth of individual trees.\n\n**\\# Trees:**\n\n-   Performance stays relatively stable across different numbers of trees, but there's a bit more variation for MAE and RMSE.\n\n-   No strong improvement after a certain number of trees.\n\n**Learning Rate (log-10):**\n\n-   Extremely small learning rates (e.g., -2.5 to -3.0) lead to worse performance, especially for MAE and RMSE.\n\n-   Mid-range values (e.g., -1.5 to -1) seem to give the best performance overall.\n\n**Tree Depth:**\n\n-   There's a clear spike in error around 9+ for MAE and RMSE.\n\n-   Lower tree depths seem to consistently perform better, especially when paired with a good learning rate.\n\n## Check the skill of the tuned model\n\n### Collect metrics:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(model_params)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 75 × 9\n   trees tree_depth learn_rate .metric .estimator   mean     n std_err .config  \n   <int>      <int>      <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>    \n 1   842          1     0.0691 mae     standard   0.105     10 0.00358 Preproce…\n 2   842          1     0.0691 rmse    standard   0.191     10 0.0120  Preproce…\n 3   842          1     0.0691 rsq     standard   0.988     10 0.00120 Preproce…\n 4   774          2     0.0143 mae     standard   0.0922    10 0.00267 Preproce…\n 5   774          2     0.0143 rmse    standard   0.162     10 0.00720 Preproce…\n 6   774          2     0.0143 rsq     standard   0.990     10 0.00164 Preproce…\n 7   282          2     0.164  mae     standard   0.0987    10 0.00352 Preproce…\n 8   282          2     0.164  rmse    standard   0.161     10 0.00699 Preproce…\n 9   282          2     0.164  rsq     standard   0.990     10 0.00142 Preproce…\n10  1220          3     0.0178 mae     standard   0.0865    10 0.00347 Preproce…\n# ℹ 65 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(model_params) %>%\n  filter(.metric == \"mae\") %>%\n  arrange(mean) %>%\n  slice_head(n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 9\n  trees tree_depth learn_rate .metric .estimator   mean     n std_err .config   \n  <int>      <int>      <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>     \n1   338          4    0.0804  mae     standard   0.0831    10 0.00326 Preproces…\n2  1290          4    0.0102  mae     standard   0.0845    10 0.00329 Preproces…\n3  1796          5    0.0548  mae     standard   0.0859    10 0.00403 Preproces…\n4  1220          3    0.0178  mae     standard   0.0865    10 0.00347 Preproces…\n5  1727          6    0.00417 mae     standard   0.0872    10 0.00400 Preproces…\n```\n\n\n:::\n:::\n\n\n\nUsing the *collect_metrics* function, we can generate a tibble showing the top 5 hyperparameter combinations ranked by lowest MAE. For each of the 5, their MAE is between 0.083 and 0.088. The tibble also shows the hyperparameters that combined to have the lowest MAE, including trees, tree depth, learn rate, and more.\n\n### Show best, by MAE:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(model_params, metric = \"mae\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 9\n  trees tree_depth learn_rate .metric .estimator   mean     n std_err .config   \n  <int>      <int>      <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>     \n1   338          4    0.0804  mae     standard   0.0831    10 0.00326 Preproces…\n2  1290          4    0.0102  mae     standard   0.0845    10 0.00329 Preproces…\n3  1796          5    0.0548  mae     standard   0.0859    10 0.00403 Preproces…\n4  1220          3    0.0178  mae     standard   0.0865    10 0.00347 Preproces…\n5  1727          6    0.00417 mae     standard   0.0872    10 0.00400 Preproces…\n```\n\n\n:::\n:::\n\n\n\nUsing the *show_best* function reveals the best 5 hyperparameter tests based on their MAE. These are tests with number of trees ranging from 338 to 1727, tree depths ranging from 3 to 6, and learn rates ranging from 0.004 to 0.08. They are all connected by their MAE value of approximately 0.085 (0.083-0.087).\n\n### Select and save best:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhp_best <- select_best(model_params, metric = \"mae\")\n```\n:::\n\n\n\n## Finalize the model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_wf <- finalize_workflow(\n  wf_tune,\n  hp_best\n)\n\nfinal_fit <- last_fit(final_wf, split = camels_split)\n```\n:::\n\n\n\n## Final Model Verification\n\n### Use last_fit\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit <- last_fit(\n  final_wf,\n  camels_split\n)\n```\n:::\n\n\n\n### Use collect_metrics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(final_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.146 Preprocessor1_Model1\n2 rsq     standard       0.989 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n### Interpretation\n\n**RMSE = 0.1462**\n\nThis means that, on average, the predictions are off by about 0.15 units. In regression, a lower RMSE indicates better model performance, so this is a low RMSE overall.\n\n**RSQ = 0.9893**\n\nThis means that about 98.93% of the variance in the actual test data is explained by the model. This is extremely high, which suggests the model is doing a great job of predicting the data.\n\n**Compared to training data**\n\nThe best training MAE's were around the 0.004 - 0.08 range, depending on hyperparameters. If the training RMSE was significantly lower than 0.146, the model may be slightly overfitting, but with an R-squared value of 0.989, it's likely still performing very well overall.\n\n### Use collect_predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit <- last_fit(final_wf, split = camels_split)\n\nfinal_preds <- collect_predictions(final_fit)\n```\n:::\n\n\n\n### Create scatter plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(final_preds, aes(x = .pred, y = q_mean)) +\n  geom_point(aes(color = .pred), alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"midnightblue\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"darkred\") +\n  scale_color_gradient(low = \"plum\", high = \"slateblue4\") +\n  labs(\n    title = \"Predicted vs Actual Values\",\n    x = \"Predicted\",\n    y = \"Actual\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](hyperparameter-tuning_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\n## Building a Map\n\n### Use fit\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit <- fit(final_wf, data = camels_clean)\n```\n:::\n\n\n\n### Use augment\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_aug <- augment(final_fit, new_data = camels_clean)\n\nprint(final_aug)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 670 × 59\n   .pred gauge_id gauge_lat gauge_lon p_mean pet_mean p_seasonality frac_snow\n   <dbl> <chr>        <dbl>     <dbl>  <dbl>    <dbl>         <dbl>     <dbl>\n 1  1.71 01013500      47.2     -68.6   3.13     1.97        0.188      0.313\n 2  2.16 01022500      44.6     -67.9   3.61     2.12       -0.115      0.245\n 3  1.82 01030500      45.5     -68.3   3.27     2.04        0.0474     0.277\n 4  2.04 01031500      45.2     -69.3   3.52     2.07        0.104      0.292\n 5  2.18 01047000      44.9     -70.0   3.32     2.09        0.148      0.280\n 6  2.41 01052500      44.9     -71.1   3.73     2.10        0.152      0.353\n 7  2.75 01054200      44.4     -71.0   4.07     2.13        0.105      0.300\n 8  2.26 01055000      44.6     -70.6   3.49     2.09        0.167      0.306\n 9  1.82 01057000      44.3     -70.5   3.57     2.13        0.0791     0.251\n10  1.71 01073000      43.1     -71.0   3.50     2.21        0.0304     0.175\n# ℹ 660 more rows\n# ℹ 51 more variables: aridity <dbl>, high_prec_freq <dbl>,\n#   high_prec_dur <dbl>, high_prec_timing <chr>, low_prec_freq <dbl>,\n#   low_prec_dur <dbl>, low_prec_timing <chr>, geol_1st_class <chr>,\n#   glim_1st_class_frac <dbl>, geol_2nd_class <chr>, glim_2nd_class_frac <dbl>,\n#   carbonate_rocks_frac <dbl>, geol_porostiy <dbl>, geol_permeability <dbl>,\n#   soil_depth_pelletier <dbl>, soil_depth_statsgo <dbl>, …\n```\n\n\n:::\n:::\n\n\n\n### Use mutate\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_aug <- final_aug %>%\n  mutate(residual = (q_mean - .pred)^2)\n```\n:::\n\n\n\n### Create a map of predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_pred <- ggplot(final_aug, aes(x = gauge_lon, y = gauge_lat)) +\n  geom_point(aes(color = .pred), size = 1.5, alpha = 0.7) +\n  scale_color_gradient(low = \"plum1\", high = \"slateblue4\") +\n  labs(title = \"Predicted Streamflow\", color = \"Prediction\") +\n  theme_minimal()\n```\n:::\n\n\n\n### Create a map of residuals\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_resid <- ggplot(final_aug, aes(x = gauge_lon, y = gauge_lat)) +\n  geom_point(aes(color = residual), size = 1.5, alpha = 0.7) +\n  scale_color_gradient(low = \"antiquewhite\", high = \"seagreen4\") +\n  labs(title = \"Squared Residuals\", color = \"Residual\") +\n  theme_minimal()\n```\n:::\n\n\n\n### Combine the two maps\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\nmap_pred + map_resid\n```\n\n::: {.cell-output-display}\n![](hyperparameter-tuning_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "hyperparameter-tuning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}